# Override default variables by putting them in this file
standalone_host: <public ip>
# since this is an AIO deployment, lets take advantage of large mtu similar to loopback
# this feature is not merged into the main dev-install distro (yet)
dummy_mtu: 65536
rhos_release: 16.2
rhsm_enabled: true
redhat_registry_credentials:
  "userid": "token"
rhsm_org_id: 999999999999
rhsm_activation_key: 7777-7777-7777-7777-7777
hostname: rhosp-dcib
clouddomain: example.com
localcloudname: rhosp-dcib
octavia_enabled: true
manila_enabled: true
ceph_enabled: true
# this toggles lvm syntax for the ceph jinja2 template
# this feature is not merged into the main dev-install distro (yet)
ceph_devices_lvm: true

ceph_devices:
  - data: ceph-osd-0
    data_vg: samsung_983e
    db: ceph-osd-0-db
    db_vg: intel_p5800x
    wal: ceph-osd-0-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-1
    data_vg: samsung_983e
    db: ceph-osd-1-db
    db_vg: intel_p5800x
    wal: ceph-osd-1-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-2
    data_vg: samsung_983e
    db: ceph-osd-2-db
    db_vg: intel_p5800x
    wal: ceph-osd-2-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-3
    data_vg: samsung_983e
    db: ceph-osd-3-db
    db_vg: intel_p5800x
    wal: ceph-osd-3-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-4
    data_vg: samsung_983e
    db: ceph-osd-4-db
    db_vg: intel_p5800x
    wal: ceph-osd-4-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-5
    data_vg: samsung_983e
    db: ceph-osd-5-db
    db_vg: intel_p5800x
    wal: ceph-osd-5-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-6
    data_vg: samsung_983e
    db: ceph-osd-6-db
    db_vg: intel_p5800x
    wal: ceph-osd-6-wal
    wal_vg: intel_p5800x
  - data: ceph-osd-7
    data_vg: samsung_983e
    db: ceph-osd-7-db
    db_vg: intel_p5800x
    wal: ceph-osd-7-wal
    wal_vg: intel_p5800x

virt_release: av
rhsm_release: 8.4
rhsm_container_tools_version: 3.0
rhsm_repos:
  - rhel-8-for-x86_64-baseos-eus-rpms
  - rhel-8-for-x86_64-appstream-eus-rpms
  - rhel-8-for-x86_64-highavailability-eus-rpms
  - ansible-2.9-for-rhel-8-x86_64-rpms
  - openstack-16.2-for-rhel-8-x86_64-rpms
  - fast-datapath-for-rhel-8-x86_64-rpms
  - advanced-virt-for-rhel-8-x86_64-rpms
  - rhceph-4-tools-for-rhel-8-x86_64-rpms

# The dual AMD EPYC 75F3 32-Core Processor has the followng numa topology (128vcpu):
#
# NUMA node0 CPU(s):   0-3,64-67
# NUMA node1 CPU(s):   4-7,68-71
# NUMA node2 CPU(s):   8-11,72-75
# NUMA node3 CPU(s):   12-15,76-79
# NUMA node4 CPU(s):   16-19,80-83
# NUMA node5 CPU(s):   20-23,84-87
# NUMA node6 CPU(s):   24-27,88-91
# NUMA node7 CPU(s):   28-31,92-95
# NUMA node8 CPU(s):   32-35,96-99
# NUMA node9 CPU(s):   36-39,100-103
# NUMA node10 CPU(s):  40-43,104-107
# NUMA node11 CPU(s):  44-47,108-111
# NUMA node12 CPU(s):  48-51,112-115
# NUMA node13 CPU(s):  52-55,116-119
# NUMA node14 CPU(s):  56-59,120-123
# NUMA node15 CPU(s):  60-63,124-127
#
# We will isolate the host os and cloud internals on numa nodes0 through nodes3 (32 vcpu)
# the remaining we will use for instance scheduling
# kernel_args: "amd_iommu=on iommu=pt nohz=off processor.max_cstate=0 nosoftlockup default_hugepagez=1G hugepagesz=1G hugepages=384 isolcpus=16-19,80-83,20-23,84-87,24-27,88-91,28-31,92-95,32-35,96-99,36-39,100-103,40-43,104-107,44-47,108-111,48-51,112-115,52-55,116-119,56-59,120-123,60-63,124-127"
# hugepages=384 facilitates 192x 2gb, 96x 4gb, 48x 8gb instances in performance memory alignment
kernel_args: "amd_iommu=on iommu=pt nohz=off processor.max_cstate=0 nosoftlockup default_hugepagez=1G hugepagesz=1G hugepages=384"
cip_config:
  - set:
      ceph_alertmanager_image: ose-prometheus-alertmanager
      ceph_alertmanager_namespace: registry.redhat.io/openshift4
      ceph_alertmanager_tag: 4.1
      ceph_grafana_image: rhceph-4-dashboard-rhel8
      ceph_grafana_namespace: registry.redhat.io/rhceph
      ceph_grafana_tag: 4
      ceph_image: rhceph-4-rhel8
      ceph_namespace: registry.redhat.io/rhceph
      ceph_node_exporter_image: ose-prometheus-node-exporter
      ceph_node_exporter_namespace: registry.redhat.io/openshift4
      ceph_node_exporter_tag: v4.1
      ceph_prometheus_image: ose-prometheus
      ceph_prometheus_namespace: registry.redhat.io/openshift4
      ceph_prometheus_tag: 4.1
      ceph_tag: latest
      name_prefix: openstack-
      name_suffix: ''
      namespace: registry.redhat.io/rhosp-rhel8
      neutron_driver: ovn
      rhel_containers: false
      tag: '16.2'
    tag_from_label: '{version}-{release}'

standalone_extra_config:
  neutron::plugins::ml2::ovn::ovn_emit_need_to_frag: true
  cinder::api::default_volume_type: "stripe-vdo"
  cinder_user_enabled_backends:
    - stripe-vdo
    - stripe-performance
    - stripe-ultra

  cinder::config::cinder_config:
    backend_defaults/lvm_type:
      value: default

    stripe-vdo/volume_driver:
      value: cinder.volume.drivers.lvm.LVMVolumeDriver
    stripe-vdo/volume_group:
      value: stripe-vdo
    stripe-vdo/volume_backend_name:
      value: vdo

    stripe-performance/volume_driver:
      value: cinder.volume.drivers.lvm.LVMVolumeDriver
    stripe-performance/volume_group:
      value: stripe-performance
    stripe-performance/volume_backend_name:
      value: performance

    stripe-ultra/volume_driver:
      value: cinder.volume.drivers.lvm.LVMVolumeDriver
    stripe-ultra/volume_group:
      value: stripe-ultra
    stripeultra/volume_backend_name:
      value: ultra

